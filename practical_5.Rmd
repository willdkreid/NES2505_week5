---
title: 'Practical 5: Unconstrained Ordination'
subtitle: "NES2505"
output:
  html_document: default
  pdf_document: 
  word_document:
     reference_docx: template.docx
---


```{r setup, include=FALSE}
library(mosaic)
library(vegan)
library(tibble)
library(ggpubr)
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction
Until now, all the 'response' data you have looked at consists of a single column of data. Examples you have looked at include 'growth', 'presence or absence of bacterial colonies' etc. You can easily summarise data that consists of a single column of numbers using a mean, median, standard deviation. But what if your response variable consists of lots of columns of data? For example:

* instead of a single column telling you the total number of species found at 20 sites, you have a table of 20 rows, with 17 columns. Each cell entry in the table shows the abundance of a particular species (column) at that site (row)
* instead of single column giving an indication of the number of bacteria colonies you have multiple columns, from qPCR sequencing, giving information about the genetic information (each column) for each bacterial colony (each row)

Such tables of data are harder to understand using means and standard deviations. Whilst you can still calculate these, you end up with lots of information to digest. For example, you can still calculate the mean abundance of each of your 17 species at all your 20 sites. But this does not allow you to answer key questions, e.g.

* which sites are most similar in their species composition?
* which species regularly co-occur with each other?
* which bacterial strains have the most similar genomes based on the qPCR results?

Analyses of tables of data with multiple columns, rather than a single column, are sometimes called "multivariate analyses", to distinguish them from the "univariate analyses" you covered in Practicals 1 to 4. These tables can be formally analysed with the aid of explanatory data, which we will cover in Practical 6. In this practical we will focus on methods to simplify the data, so that they can be easily visualised, and relationships between rows and columns of your input data better understood. These methods are known as **unconstrained ordination**. Here we say "unconstrained" because we do not directly incorporate explanatory variables into the analysis, although after the initial analysis has been done, we can use them to aid interpretation. We say "ordination" because we can create metrics to order the rows and columns of your input data on the basis of their similarity.

## Types of ordination
Ordination is a way of arranging rows in your table (e.g. sites or samples) along gradients in such a way that we can try and explain patterns of variation within the noise. Hopefully we can explain these gradients on the basis of some explanatory data **after** we have done the ordination (hence **unconstrained** ordination). Linear methods (**PCA**) assume the observations respond roughly linearly along these gradients. Unimodal methods (**CA**) assume the observations will respond non-linearly. Distance-based models (**NMDS**) use the relative rankings of your sites along each axis.

Unconstrained ordination is a useful tool for determining whether there is a relationship between multiple **response variables**. Through measuring the similarity in their **composition**, it is easier to compare samples. When the composition of samples is summarised on a continuous scale it is referred to as an **ordination**, whereas on a categorical scale it is known as **classification** or **clustering**.

The main aims of this practical are to learn more on the use, interpretation and graphical visualisation of unconstrained ordination. Specific objectives are to learn how to use:

1. Principal components analysis (PCA), a "linear" method
2. Correspondence analysis (CA), a "unimodal" method
3. Non-metric multidimensional scaling (NMDS), a "rank-based" method

Please complete [this interactive website](https://naturalandenvironmentalscience.shinyapps.io/NES2505Unconstrained/) before beginning the practical.

## Setting up for the practical  
First, go to Canvas and download the **dune.csv** data set and save it to your `Data` folder within your NES2505 project folder. Now navigate to your NES2505 folder and open up your existing project for this course in RStudio, as you have done in previous practicals. On the main RStudio menu, click on **File -> New File -> R script** and save it to your current working directory (the location of your Bio2020 project) as **Practical_5.R** 

## Installing and Loading Packages
Practicals 5 and 6 make use of the `vegan` R packages. The `vegan` package was originally written for vegetation analysis, but is now used in microbiology, phylogenetics, animal ecology etc. There is plenty of online material about the package, but if using Google to search for it, please search for 'R vegan multivariate' rather than just 'vegan' otherwise Google will return food recipes!

## Example datasets for analysis  
Import the `dune.csv` file you downloaded at the very beginning of the practical into R using the `read.csv()` function. This practical assumes you have save your file in the `Data` subfolder:

```{r echo=TRUE,  message=FALSE, warning=FALSE}
dune <- read.csv("Data/dune.csv", row.names=1)
```

Vegan has some in-built data files, let's load "varespec" and "varechem".
```{r echo=TRUE,  message=FALSE, warning=FALSE}
data(varespec)
data(varechem)
```

These datasets are plant ecological ones, that are often used in online tutorials about the `vegan` package. Here each column is a species, and each row a sample. Your dataset would have the identical structure if dealing with e.g. genomic data, each column might indicate a sequence mutation insert or deletion, and each row a sample. For an example of how the `vegan` package has been extended to handle complex genetic data, including **operational taxonomic units** (OTUs), output from **mothur** and **QIIME** look at the excellent [phyloseq](https://joey711.github.io/phyloseq/) R package.
 
## Summary Statistics 
It is always good to start by observing the data and running some summary statistics.

```{r echo=TRUE, include=TRUE, results = 'hide'}
summary(varespec)

summary(dune)
```

However, you will notice that this output is rather large, hence why it is not included here. With big datasets it can be easier to observe the data by simply using **head()** to get a snapshot of the data, and then **nrow()**, **ncol()**, **rownames()**, and **colnames()** to explore the data a bit more.

You can also use the `View()` function (capital 'V') within RStudio to show it as a grid.

```{r echo = TRUE, eval=FALSE}
# Show first few lines and check dimensions
head(varespec)
nrow(varespec)
ncol(varespec)

# Column names are abbreviated Latin names
rownames(varespec)
colnames(varespec)

# Similarly for dune dataset
head(dune)
nrow(dune)
ncol(dune)
rownames(dune)
colnames(dune)

# Display in spreadsheet-like tab
View(dune) # not view(dune)
View(varespec)
```

# 1. Principal Components Analysis
## 1.1 PCA analysis and summary of results

In the [interactive webpage](https://naturalandenvironmentalscience.shinyapps.io/NES2505Unconstrained/#section-principal-components-analysis), we went through doing a PCA using data from a sand dune system and we also saw the problems that may occur when you have the arch or horseshoe effect . Although **unimodal methods** are a good alternative, you can implement transformations using the `decostand()` function. Here, we use `scale = TRUE` to give the data **unit variance**, and the **hellinger transformation**, which works well with environmental and species data.

We are going to use the dune data set again to undertake your analysis. We use the `rda()` function to do the PCA.

```{r}
# Undertake the PCA ordination
varespec_pca <- rda(decostand(varespec, method = "hellinger"), scale = TRUE)
varespec_pca

```

Output explained:

- **Call** - This is a reminder of how you fitted the PCA.
- **Inertia** - Total inertia is a measure of total variance.
- **Unconstrained** - How many correlations have been explained by the ordination axes. In this case, there is one less than the number of variables (we have 24 rows of data).
- **Eigenvalues** - Measure of the amount of variance that is explained by each of the axes
- **PC1** - The first axes, and will explain the largest amount of variation of all the axes.

To see all 23 axes, use **summary()**. This is a large output so it is not included here.

```{r echo=TRUE, include=TRUE, results = 'hide'}
summary(varespec_pca)
```

This produces a lot of output, but don't worry, it is less fearsome than it looks!

Output explained:

- The top 6 lines are the same as the PCA output
- **Eigenvalues** are the amount of inertia (variance) explained by each axis. If added up for all 23 axes, it will be the same as the total inertia.
- **Proportion explained** is the amount of total variance explained, and if added up for all 23 axes will equal 1.00. You can see that the proportion explained is highest for PC1, next highest for PC2 and so on. The last PC, PC23, explains virtually nothing.
- **Cumulative proportion** is the cumulative total of the proportion of each axes. While **PC1** explains 0.5384 and **PC2** explains 0.2543 of the variance, their *cumulative* proportion is 0.7927 because it is the total amount of variance they explain together.
- **Species and Site scores** - the relative weight each species and/or site has within each axes.

The lines that are most useful to focus on are the Proportion explained, and Cumulative proportion. PCA tries to "squash" all the original data from 24 dimensions (we have 44 columns or species) into 2 dimensions. Why? You cannot plot a 44 dimensional graph (or if you know how to do so, please apply for a Nobel Prize!). But you can plot a 2 dimensional graph. So if you can squash as much of the 'information' in your original table of data into 2 new variables that summarise your species that makes life easier.

## 1.2 Visualise your PCA results
In general we are only interested in PC1 and PC2. Very rarely, there might be useful information in PC3 so you may want to check it, although in it is usually of little value. Generally, the first two axes tend to explain the majority of the variation, so these are usually plotted. 

```{r}
# both sites and species
plot(varespec_pca)

# just sites; note how you control x and y axis limits
plot(varespec_pca, display = "sites")

# just species
plot(varespec_pca, display="species")

```

When this dataset was analysed [initial on the interactive website with PCA](https://naturalandenvironmentalscience.shinyapps.io/NES2505Unconstrained/#section-pca-arch-effect) it suffered from a severe 'arch effect'. You will notice we do not have an arch effect now, even though we are still using PCA. This is because we used `decostand()` to standardise our data, and reduce the effects of outliers. Overall PC1 and PC2 explain less variation than before but the arch effect has been removed so the graph is easier to understand.

Species that are close together tend to co-occur at particular sites, and sites that are close together have similar species composition. Species that are close to the zero-zero lines are more common across all sites. You will notice that due to overplotting it is difficult to make out some species/sites. One way round this is to adjust the x- and y-axis so that you can look more closely at the species scores. We can do this by first creating an empty plot using `type = "n"` and setting the limits for the x- and y-axes using `xlim = c()` and `ylim = c()`.

```{r echo=TRUE, include=TRUE, eval=FALSE}
# Create an empty plot first then add species and site scores as text
plot(varespec_pca, type = "n")
text(varespec_pca, display = "species", cex = 0.7, col = "red")
text(varespec_pca, display = "sites", cex = 0.7, col = "blue")

# Create the same plot again but this time add limits to the x- and y-axes
plot(varespec_pca, xlim=c(-2,2), ylim=c(-1,1),type = "n")
text(varespec_pca, display = "species", cex = 0.7, col = "red")
text(varespec_pca, display = "sites", cex = 0.7, col = "blue")

```

You should be able to see some of the species names and begin to be able to interpret the PCA plot.

## 1.3 Relate PCA to explanatory variables
In unconstrained methods you undertake the ordination analysis first, and then relate your results back to any explanatory variables later. First, you need to obtain the sample PC scores used in your earlier plot. These are `choices = 1` for PC1 (x-axis in PC1 v PC2 plot) or `choices = 2` for PC2.

After having obtained these scores, you can see if there is any relationship with the explanatory data (in this case soil chemistry), to help you understand what external factors might be affecting the results. The data set`varechem` contains the environmental data. You can use `names()` to find out which variables have been collected. Look at the relationship between PC1 and pH first

```{r}
# find the names of the variables which are in varechem
names(varechem)

# Extract the PC1 site scores
varespec_pc1 <- scores(varespec_pca, display="sites", choices = 1)

# Create plot (point plot because pH is continuous data)
# We put soil pH on the horizontal x-axis as we assume it determines the
# plant species composition
gf_point(varespec_pc1 ~ pH, data=varechem)
```

Think about how the PC1 scores relate to the environment, and try the same with PC2, or with some of the other soil chemistry variables.

# 2. Correspondence Analysis
## 2.1 CA analysis and summary of results
Correspondence analysis (CA) is a weighted form of PCA that can fit non-linear responses. The weighting means the analysis is on the relative composition instead of absolute values. It is similar to PCA in that species that are close together tend to co-occur, and sites that are close together tend to have similar species composition. To undertaken a CA we use the function `cca()`. 

```{r}
dune_ca <- cca(dune)
dune_ca
```

Output explained (similar to PCA):

- **Call** - This is a reminder of how you fitted the CA.
- **Inertia** - Total inertia is a measure of total variance.
- **Unconstrained** - How many correlations have been explained by the ordination axes. In this case, there is one less than the number of variables.
- **Eigenvalues** - Measure of the amount of variance that is explained by each of the axes

To see the full output, use **summary()**. Again, this is a large output so it is not shown here.

```{r echo=TRUE, include=TRUE, results = 'hide'}
summary(dune_ca)
```

Output explained (similar to PCA):

- **CA1** - The first axes, and will explain the largest amount of variation of all the axes.
- The top 6 lines are the same as the CA output
- **Eigenvalues** are the amount of inertia (variance) explained by each axis. If added up for all 19 axes, it will be the same as the total inertia.
- **Proportion explained** is the amount of total variance explained, and if added up for all 19 axes will equal 1.00.
- **Cumulative proportion** is the cumulative total of the proportion of each axes. 
- **Species and Site scores** - the relative weight each species and/or site has within each axes.

What is the proportion of variance explained by CA1 and CA2 and what is the cumulative proportion explained of both CA1 and CA2? Remember that the interpretation of this output is similar to PCA so go back and check how to interpet **Proportion explained** and **Cumulative proportion**. 

You can see that whilst PCA and CA differ in their underlying philosophy (linear vs unimodel) the summary of the results of the analyses is similar in interpretation.

## 2.2 Visualise your CA results
Visualisation is very similar to that for PCA, and is most easily accessed via `plot()` which is compatible with the plotting system you have already used.

```{r}
# All sites or samples
plot(dune_ca)

# Plot CA1 and CA2 of the samples
plot(dune_ca, display="sites") 

# Plot CA1 and CA2 of the species
plot(dune_ca, display="species") 
```

Again, the species or attribute plot can get very cluttered depending on your dataset, so you might find it easier to alter the axes to look more closely at certain areas of the plot. Edit the code below to change the limits for the x- and y-axes. There are other errors in the code so be careful, especially when it comes to copying and pasting code you've already used!

```
# Create the same plot again but this time add limits to the x- and y-axes
plot(dune_ca, xlim=c(), ylim=c(),type = "n")
text(dune_ca, dislay = "species", cex = 0.7, col = "")
text(varespec_pca, display = "sites", cex = 7, col = "blue")
```

From the plot, can you figure out which species are most associated with sites 4, 5 and 19 and which species are likely to be found across all sites?

# 3. Non-metric Multidimensional Scaling 
## 3.1 NMDS background and analysis
NMDS creates the ordination using the rank order of your sites along each axis, rather than absolute multi-dimensional distances. NMDS only makes sure that the points further apart are still further apart in NMDS space than the “closer together” points, so it does not preserve the actual distance. It determines these rankings from pairwise similarity scores between each pair of samples in turn, and analysing the resultant table. The **metaMDS()** function by default uses the Bray-Curtis similarity measure which is robust for most data. The algorithm has to run multiple times to find the best solution. If needed, it will automatically standardise your data by either square-root and/or ‘Wisconsin’ standardisation, which standardises species to equal maxima and sites to equal totals. You may see this displayed in the output as the model runs.


```{r}
dune_nmds <- metaMDS(dune)
```

Output explained:

NMDS uses many random starts (20 each time) and looks for the fits with the lowest stress. It will only conclude that a solution has been reached when the solutions with the lowest stress are similar. It also fits the NMDS for 1,2,3...etc dimensions, and stops after a sudden drop in stress is observed. Sometimes the NMDS cannot find a solution on the first try like it has here (you will get back **no convergent solutions**), which is why we've saved the output as the object **dune_nmds**. To run the NMDS on the same data again: 

dune_nmds_2 <- metaMDS(dune, previous.best = dune_nmds)

The **previous.best** argument passes in a previous fit of NMDS and will run another 20 random starts (making the total number of random starts 40). However, you do not need to run this on the dune dataset as we have already found a convergent solution. 

Check the output of the nmds:
```{r}
# Check the output
print(dune_nmds)
```

Output explained:

- **Call** - This is a reminder of how you fitted the NMDS.
- **Data** - This is the type of data used, species in this case.
- **Distance** - the dissimilarity metric, in this case the default is *Bray-Curtis*.
- **Dimensions** - the number of dimensions with the least stress. 
- **Two convergent solutions found after 20 tries** - the NMDS did 20 random starts and found 2 solutions that were very similar to one another. If you ran the NMDS twice, it would say **...after 40 tries** here.

## 3.2 Visualise your NMDS results
Again you can use the `plot()` function to visualise the results. Sometimes, it easier to  plot the samples and species separately, and compare the two graphs.

```{r}
# Plot the NMDS sample (site) and attribute (species) scores in the same plot
plot(dune_nmds)

# Plot the NMDS sample (site) and attribute (species) scores in different plots
plot(dune_nmds, display="sites")
plot(dune_nmds, display="species")
```

If you wish to compare two graphs side-by-side, you will have to set the plotting window using `par()`.

```{r}
# Set the plotting window so that the NMDS sample (site) and attribute (species) scores side-by-side so that you have one row and two columns
par(mfrow=c(1,2))

# plot the NMDS scores using text so you can see what the points represent
plot(dune_nmds, display="sites", type = "n")
text(dune_nmds, display = "sites", cex = 0.7, col = "blue")
plot(dune_nmds, display="species", type = "n")
text(dune_nmds, display = "species", cex = 0.7, col = "red")
par(mfrow=c(1,1))

```

Comparing between plots makes it easier to understand relationships. For example from these plots you can see that the species Elymrepe and Cirsarve are probably most common at site 1. Species Airaprae, Empenigr and Hyporadi are probably commonly found together. They are particularly characteristic of sites 17 and 19 which also have high NMDS2 scores.

## 3.4 Enhancing visualisations

The ordination plots we have produced so far have plotted the species and site scores. We have established that the those species that are closer together in ordination space are more likely to be found together within a sample. Likewise when sites are found close together they will contain very similar species composition. However, the ordination plots that we have produced so far do not give us an indication if there are environmental factors which explain any groupings that we see within these plots.

`vegan` has some additional functionality which allows us to add:

* convex hulls: draws a line around the extreme points for a group of site scores to create a polygon
* ellipses: attempts to draw an ellipse around the extreme points for a group of sites scores
* spiders: draws lines from each site score to a group's centroid position

We can have a look at these bay carrying on looking at the dune data set. if you haven't already done so make sure the `dune.env` data has been loaded into your environment. You should see it in the Environment section of the RStudio window. If not the `dune.env` data set comes built into `vegan` so you can use the `data()` command to load it. We need to use a combination of the ordination you have already done which contains the species and site scores and the `dune.env` data to start customising our plot to get more information ordination plot. 

```{r}
# load the data.env data set if you have not already done so
data(dune.env)

# plot the empty ordination first
plot(dune_nmds, display = "sites", type = "n")

# add the site scores and use the Management variable to colour the different 
points(dune_nmds,display = "sites", cex = 0.7, col = dune.env$Management)

# add the convex hull
ordihull(dune_nmds, dune.env$Management, label = T, col = 1:4, lwd=1)

```

We can see that the site scores are actually forming some groups in the ordination plot. Those sites are coloured green and relate to the nature conservation management (NM) are grouping together and are separate from BF (biological farming), HF (hobby-farming) and standard farming (SF).

We can also add other features to the ordination plot which can help with visualisation of the groupings. The first is an ellipse which is essentially a curved version of the convex hull. You will want to try and change the axes limits to fit in the whole ellipses.

```{r}
# plot the empty ordination first
plot(dune_nmds, display = "sites", type = "n")

# add the site scores and use the Management variable to colour the different 
points(dune_nmds,display = "sites", cex = 0.7, col = dune.env$Management)

# add the ellipse
ordiellipse(dune_nmds, dune.env$Management, label = TRUE, kind = "ehull", col = 1:4, lwd=1)

```

In the examples so far, we have added colour to help visualise that different sites come from different management measures but we don't know which specific sites are related to each management measure. We can use a spiderplot to help understand this because it will identify which group each site score is assigned to and then indicate that points distance to the centroid (labelled with text). The centroid is the mean position of all the points on a surface. The spiderplot gives us an indication of how variable the scatter is for each of the site scores within a group.

```{r}
# plot the empty ordination first
plot(dune_nmds, display = "sites", type = "n")

# add the site scores and use the Management variable to colour the different 
text(dune_nmds,display = "sites")

# add the spider
ordispider(dune_nmds, dune.env$Management, label = "TRUE", col = 1:4, lwd=1)

```

The different visualisation techniques allow us to see whether there are groupings in our data. The addition of environmental data which can help us explain these groups by incorporating that information into the visualisation helps to communicate those groupings to the reader. Thinking back to the stress value for the NMDS, how confident are you with the groupings that you see in the ordination plot and why?

You can use some of these techniques with PCA and CA as well.

# 4.  Summary
Unconstrained methods provide useful ways of summarising tables of data from many different biological disciplines. They are worth considering if you have roughly 10 or more columns of data. This is particularly common in genomics and ecological studies, where multiple genes or species may be recorded for each sample or site. Doing lots of separate analyses on each column individually would be too time-consuming. These ordination methods also help you to identify **relationships** between variables, so that you can determine which samples (or sites) and which genes (or species) are most similar to each other.

There is a lot more that you can do with the `vegan` package when it comes to analysing community data. On the R website where the package is hosted, there is more information about the capabilities of `vegan` in the [vignettes section](https://cran.r-project.org/web/packages/vegan/index.html). It is worthwhile having a look at these if you are interested to learn more.

